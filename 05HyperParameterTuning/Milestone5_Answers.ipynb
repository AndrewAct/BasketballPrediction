{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Milestone5_Answers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBiGaSWcOfNNg+KbrS6wt0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAmlq-gbEXZ5",
        "colab_type": "text"
      },
      "source": [
        "# Hyper Parameter Tuning  \n",
        "\n",
        "Unless you are lucky, your first model architecture won't be perfect. This milestone is attempting to find the optimal parameters for your neural network. Below, I will try with different layers, different neuron counts, and different activation functions.  \n",
        "\n",
        "First, I am going to try different amounts of neurons in the layers. From there, I will take the winner and change the activation function. Once I have that picked out, I will change the optimizer.\n",
        "\n",
        "### Layers and Neurons  \n",
        "We can adjust the complexity of the model but varying the number of layers and the neurons inside of them. Our only limitation is that our input layer has to match our data and our output layer needs to give us a number.  \n",
        "\n",
        "One thing to keep in mind is that the more complexity of your model the more data it will take to generalize it. This is because the network has so many paths through it that it will give the exact result of game versus having multiple games go through that same path.\n",
        "\n",
        "### Activation Function  \n",
        "The activation function determines how each neuron is adjusted based on the error. In our case, if the expected result was 5 and the actual was 3 that means the activation function will pass in the error into its function and then adjust the neuron accordingly.\n",
        "\n",
        "We started with `relu` and will try `sigmoid` and `softmax`. \n",
        "\n",
        "**RELU**: Rectified Linear Unit. It is a very quickly converging. This function will return a positive value. Equation: $$relu(x) = max(x,0)$$  \n",
        "**Softmax**: This function will also return a postivie value but from 0 to 1. It converts a real vector to a vector of categorical probabilities. Equation: $$softmax(x_i) = \\frac{exp(x_i)}{\\sum_{0}^{k}exp(x_k)}$$  \n",
        "**Sigmoid**: This function will also return a postive value from 0 to 1. It is equivalent to a 2-element Softmax. Equation: $$sigmoid(x) = \\frac{1}{(1 + exp(-x))}$$  \n",
        "\n",
        "Available Keras activation functions: [https://keras.io/api/layers/activations/](https://keras.io/api/layers/activations/).  \n",
        "\n",
        "### Optimizer  \n",
        "**RMSProp** [Link](https://keras.io/api/optimizers/rmsprop/): Maintains a moving average of the squares of the gradients and divides the gradient by the root of this average.  \n",
        "**SDG (stochastic gradient descent)** [Link](https://keras.io/api/optimizers/sgd/): A basic gradient descent optimizer.  \n",
        "**Adam** [Link](https://keras.io/api/optimizers/adam/): Adam is based on SDG but uses adaptive estimation of the first and second order moments.  \n",
        "**Adamax** [Link](https://keras.io/api/optimizers/adamax/): Adamax is a variation of Adam based on the infinity norm.    \n",
        "\n",
        "Available Keras optimizers: [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wedi_5w_MVJv",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShLCSDZwMW3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAyEASD7MaKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cf8d1e01-68ec-4d37-e371-fade2567dae9"
      },
      "source": [
        "#Load the data set from the last milestone 1\n",
        "column_names = ['Date','HomeTeam','HomeScore','AwayTeam','AwayScore',\n",
        "                'HomeScoreAverage','HomeDefenseAverage','AwayScoreAverage','AwayDefenseAverage',\n",
        "                'Result']\n",
        "\n",
        "games_csv = 'https://liveproject-resources.s3.amazonaws.com/other/deeplearningbasketballscores/Games-Calculated.csv'\n",
        "all_data = pd.read_csv(games_csv, header=None, names=column_names)\n",
        "\n",
        "# Drop the columns that we are NOT going to train on\n",
        "all_data.drop(['Date','HomeTeam','HomeScore','AwayTeam','AwayScore'], axis=1, inplace=True)\n",
        "all_data.tail()\n",
        "\n",
        "#Break it into 80/20 splits\n",
        "train = all_data.sample(frac=0.8, random_state=0)\n",
        "test = all_data.drop(train.index)\n",
        "print('Training Size: %s' % train.shape[0])\n",
        "print('Testing Size: %s' % test.shape[0])\n",
        "\n",
        "#Create the labels\n",
        "train_labels = train.pop('Result')\n",
        "test_labels = test.pop('Result')\n",
        "\n",
        "# Normalize the data\n",
        "mean = train.mean(axis=0)\n",
        "train_data = train - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "test_data = test - mean\n",
        "test_data /= std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Size: 16128\n",
            "Testing Size: 4032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiLfL1QjMiP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This method will be used in place of the normal output. This is cleaner in my opinion\n",
        "class PrintDoc(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % 10 == 0: print('')\n",
        "    print('.', end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qc9-7TWMSvR",
        "colab_type": "text"
      },
      "source": [
        "## Milestone 3 Network  \n",
        "\n",
        "First, I am going to grab the network I created in milestone 3 and list the mean absolute error from the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZN_16m7EPX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Build_Model_Milestone3():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation='relu', input_shape=[4]),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.RMSprop()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aclnWiGYMrnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e5eb0576-b3b3-47e6-ae4a-11be87a6447a"
      },
      "source": [
        "m3 = Build_Model_Milestone3()\n",
        "m3_history = m3.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            ".........."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F28ygbW9NliW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25828b9e-30d7-4618-a3a8-ba27bcc20f59"
      },
      "source": [
        "_, m3_mean_absolute_error, _, _ = m3.evaluate(test_data, test_labels,verbose=0)\n",
        "print('Milestone 3 model: %s' % m3_mean_absolute_error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Milestone 3 model: 7.866069316864014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "329aGrBjO-Ef",
        "colab_type": "text"
      },
      "source": [
        "## Less Neutrons  \n",
        "\n",
        "I started with 32 neutrons in each layer. I am going to adjust that down to 24, 12, and 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaX_s9y7PLTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Build_Model_24Neutrons():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(24, activation='relu', input_shape=[4]),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.RMSprop()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model\n",
        "\n",
        "def Build_Model_12Neutrons():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(12, activation='relu', input_shape=[4]),\n",
        "    keras.layers.Dense(12, activation='relu'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.RMSprop()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model\n",
        "\n",
        "def Build_Model_8Neutrons():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(8, activation='relu', input_shape=[4]),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.RMSprop()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k55aexEuPcSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "0036fafb-883c-4427-e912-f047f615a77d"
      },
      "source": [
        "# Train the networks\n",
        "m8 = Build_Model_8Neutrons()\n",
        "m8_history = m8.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])\n",
        "\n",
        "m12 = Build_Model_12Neutrons()\n",
        "m12_history = m12.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])\n",
        "\n",
        "m24 = Build_Model_24Neutrons()\n",
        "m24_history = m24.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            ".........."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5DQsTpbPuiy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e0ce4ab7-ec0b-4b43-bb06-184574d4ae84"
      },
      "source": [
        "# Grab Their Results\n",
        "_, m8_mean_absolute_error, _, _ = m8.evaluate(test_data, test_labels,verbose=0)\n",
        "print('8 Neurons model: %s' % m8_mean_absolute_error)\n",
        "\n",
        "_, m12_mean_absolute_error, _, _ = m12.evaluate(test_data, test_labels,verbose=0)\n",
        "print('12 Neurons model: %s' % m12_mean_absolute_error)\n",
        "\n",
        "_, m24_mean_absolute_error, _, _ = m24.evaluate(test_data, test_labels,verbose=0)\n",
        "print('24 Neurons model: %s' % m24_mean_absolute_error)\n",
        "\n",
        "print('Milestone 3 model: %s' % m3_mean_absolute_error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 Neurons model: 7.877933025360107\n",
            "12 Neurons model: 7.877676010131836\n",
            "24 Neurons model: 7.846304893493652\n",
            "Milestone 3 model: 7.866069316864014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gjfvyPvR6jd",
        "colab_type": "text"
      },
      "source": [
        "**Winner**: 24 neurons!!  \n",
        "\n",
        "Now that we have the neuron winner we will move to changing the activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZABihouSPrs",
        "colab_type": "text"
      },
      "source": [
        "## Activation Functions  \n",
        "\n",
        "We currently have RELU (rectified linear unit). We will try `sigmoid` and `softmax`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmKWXVs2Si4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Build_Model_Sigmoid():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(24, activation='sigmoid', input_shape=[4]),\n",
        "    keras.layers.Dense(24, activation='sigmoid'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.RMSprop()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model\n",
        "\n",
        "def Build_Model_Softmax():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(24, activation='softmax', input_shape=[4]),\n",
        "    keras.layers.Dense(24, activation='softmax'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.RMSprop()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HReMirO8SxUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4bfe67b3-d06e-47ec-a146-4f798443b166"
      },
      "source": [
        "# Train the networks\n",
        "msg = Build_Model_Sigmoid()\n",
        "msg_history = msg.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])\n",
        "\n",
        "msm = Build_Model_Softmax()\n",
        "msm_history = msm.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            ".........."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXQHQNWMSx3H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cb3bcad2-4126-419a-ecb4-9549a907e62e"
      },
      "source": [
        "_, msg_mean_absolute_error, _, _ = msg.evaluate(test_data, test_labels,verbose=0)\n",
        "print('Sigmoid model: %s' % msg_mean_absolute_error)\n",
        "\n",
        "_, msm_mean_absolute_error, _, _ = msm.evaluate(test_data, test_labels,verbose=0)\n",
        "print('Softmax model: %s' % msm_mean_absolute_error)\n",
        "\n",
        "\n",
        "print('24 Neuron model: %s' % m24_mean_absolute_error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sigmoid model: 7.884239196777344\n",
            "Softmax model: 7.989224433898926\n",
            "24 Neuron model: 7.846304893493652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbBfVg9YVjpC",
        "colab_type": "text"
      },
      "source": [
        "**Winner**: Still the 24 neuron layers with the relu activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVee9pATVuBC",
        "colab_type": "text"
      },
      "source": [
        "## Optimizers  \n",
        "\n",
        "We started with RMSProp and we are going to try SDG (stochastic gradient descent), Adam, and Adamax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g9mqvhDWGpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Build_Model_SDG():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(24, activation='relu', input_shape=[4]),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.SGD()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model\n",
        "\n",
        "def Build_Model_Adam():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(24, activation='relu', input_shape=[4]),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.Adam()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model\n",
        "\n",
        "def Build_Model_Adamax():\n",
        "  model = keras.models.Sequential([\n",
        "    keras.layers.Dense(24, activation='relu', input_shape=[4]),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1)                                   \n",
        "  ])\n",
        "  \n",
        "  opt = keras.optimizers.Adamax()\n",
        "  m = [\n",
        "       keras.metrics.MeanAbsoluteError(),\n",
        "       keras.metrics.Accuracy(),\n",
        "       keras.metrics.MeanSquaredError()\n",
        "  ]\n",
        "  l = keras.losses.MeanSquaredError()\n",
        "  \n",
        "  model.compile(loss=l, optimizer=opt, metrics=m)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFuSNhBWZSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "961f2002-1095-4aaf-e8c7-adad5a5d8e49"
      },
      "source": [
        "# Train the networks\n",
        "sdg = Build_Model_SDG()\n",
        "sdg_history = sdg.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])\n",
        "\n",
        "adam = Build_Model_Adam()\n",
        "adam_history = adam.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])\n",
        "\n",
        "ada = Build_Model_Adamax()\n",
        "ada_history = ada.fit(train_data, train_labels, epochs=100, validation_split=0.2, verbose=0, callbacks=[PrintDoc()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            "..........\n",
            ".........."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnTWHq2nWmBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "72c5de31-2a11-4103-9650-787df73c48bf"
      },
      "source": [
        "_, sdg_mean_absolute_error, _, _ = sdg.evaluate(test_data, test_labels,verbose=0)\n",
        "print('SDG model: %s' % sdg_mean_absolute_error)\n",
        "\n",
        "_, adam_mean_absolute_error, _, _ = adam.evaluate(test_data, test_labels,verbose=0)\n",
        "print('Adam model: %s' % adam_mean_absolute_error)\n",
        "\n",
        "_, ada_mean_absolute_error, _, _ = ada.evaluate(test_data, test_labels,verbose=0)\n",
        "print('Adamax model: %s' % ada_mean_absolute_error)\n",
        "\n",
        "\n",
        "print('RMSProp model: %s' % m24_mean_absolute_error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SDG model: 8.956071853637695\n",
            "Adam model: 7.867661952972412\n",
            "Adamax model: 7.859252452850342\n",
            "RMSProp model: 7.846304893493652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvS4XebyaCHo",
        "colab_type": "text"
      },
      "source": [
        "## Export h5 Model  \n",
        "\n",
        "We will use the built in Keras `save` function to export the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePZFamvtaKIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the model and all it's weights\n",
        "m24.save('deeplearning-manning.h5')\n",
        "\n",
        "#Google Colab code to download\n",
        "from google.colab import files\n",
        "files.download('deeplearning-manning.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwlYH7NiYoE1",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion  \n",
        "The RMSProp activation was still the best. It turns out that after everything we did in this milestone the only change to my first network was to change the neurons from 32 to 24. But, this does show the process on how to narrow down your parameters when you are finalizing your network."
      ]
    }
  ]
}